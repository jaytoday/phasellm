const COT_DATA = {
    "1": {
        "step": 1,
        "objective": "1. Data cleaning and preprocessing:\n   - Remove rows with missing values (replace '?' with NaN and then drop the rows with NaN)\n   - Convert categorical variables with string values (like workclass, education, marital.status, occupation, relationship, race, sex, and native.country) to numerical values using one-hot encoding or label encoding.",
        "code_block": "```python\n# Replace missing values '?' with NaN\ndf.replace('?', np.nan, inplace=True)\n\n# Drop rows with missing values (NaN)\ndf.dropna(inplace=True)\n\n# One-hot encoding for categorical variables\ndf = pd.get_dummies(df, columns=['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country'], drop_first=True)\n```\n"
    },
    "2": {
        "step": 2,
        "objective": "2. Correlations:\n   - Hypothesis: Income is correlated with other continuous predictor variables in the dataset.\n   - Variables to look at: age, fnlwgt, education.num, capital.gain, capital.loss, hours.per.week.\n   - Calculate the correlation coefficient (e.g., Pearson's correlation) between income and each of the continuous predictor variables. This will help to identify the strength of the relationship between income and these variables.",
        "code_block": "```python\n# Convert income column to numerical values (0 for '<=50K', 1 for '>50K')\ndf['income'] = df['income'].apply(lambda x: 0 if x == '<=50K' else 1)\n\n# Calculate the correlation coefficient between income and continuous predictor variables\ncorrelations = df[['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week', 'income']].corr()\n\n# Display the correlation coefficients for income against other variables\nprint(correlations['income'])\n```\n"
    },
    "3": {
        "step": 3,
        "objective": "3. Feature Importance:\n   - Hypothesis: Some predictor variables may be more important in determining income than others.\n   - Train a tree-based model like a Random Forest or a Decision Tree on the dataset to predict income.\n   - Extract the feature importances from the trained model to identify the most important predictors of income. This will help to understand which variables have the most significant impact on income levels.",
        "code_block": "```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Separate the predictor variables (X) from the target variable (y)\nX = df.drop('income', axis=1)\ny = df['income']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a Random Forest model on the data\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Extract the feature importances from the trained model\nfeature_importances = rf_model.feature_importances_\n\n# Create a dataframe to display the feature importances alongside the predictor variable names\nimportance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n\n# Sort the dataframe by importance and display the results\nimportance_df = importance_df.sort_values('importance', ascending=False)\nprint(importance_df)\n```\n"
    },
    "4": {
        "step": 4,
        "objective": "4. ANOVA Test:\n   - Hypothesis: The means of income for different categories within each categorical variable are significantly different.\n   - Variables to look at: workclass, education, marital.status, occupation, relationship, race, sex, native.country\n   - Perform an ANOVA test to compare means of income for each category within each categorical variable. This will help in identifying if there are significant differences in income levels across different groups within each categorical variable.",
        "code_block": "```python\nimport scipy.stats as stats\n\n# Load the original dataframe\n# For ANOVA, we should work with the original categorical columns, not the one-hot encoded ones\noriginal_df = pd.read_csv('path/to/original/data.csv')\n\n# List of categorical variables\ncategorical_vars = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n\n# Replace missing values '?' with NaN and drop the rows with NaN\noriginal_df.replace('?', np.nan, inplace=True)\noriginal_df.dropna(inplace=True)\n\n# Convert income column to numerical values (0 for '<=50K', 1 for '>50K')\noriginal_df['income'] = original_df['income'].apply(lambda x: 0 if x == '<=50K' else 1)\n\n# Perform ANOVA test for each categorical variable\nanova_results = {}\nfor var in categorical_vars:\n    categories = list(original_df[var].unique())\n    samples = [original_df.loc[original_df[var] == cat, 'income'] for cat in categories]\n    f_value, p_value = stats.f_oneway(*samples)\n    anova_results[var] = (f_value, p_value)\n\n# Display the ANOVA results\nfor var, result in anova_results.items():\n    print(f\"{var}: F-value:{result[0]:.2f}, P-value:{result[1]:.2e}\")\n```\nNote that you should replace `'path/to/original/data.csv'` with the actual path of your CSV file containing the dataset."
    },
    "5": {
        "step": 5,
        "objective": "5. Linear Regression (on the continuous variables and encoded categorical variables):\n   - Hypothesis: There is a linear relationship between the predictor variables and income.\n   - Fit a linear regression model on the dataset with income as the target variable and all other variables as predictors.\n   - Evaluate the model performance (e.g., R-squared, RMSE) and analyze the coefficients of each predictor variable.",
        "code_block": "```python\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# Separate the predictor variables (X) from the target variable (y)\nX = df.drop('income', axis=1)\ny = df['income']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit a linear regression model on the dataset\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predict income values for the test set\ny_pred = linear_model.predict(X_test)\n\n# Evaluate the model performance\nr_squared = r2_score(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(f\"R-squared: {r_squared:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\n\n# Analyze the coefficients of each predictor variable\ncoefficients = pd.DataFrame({'feature': X.columns, 'coefficient': linear_model.coef_})\ncoefficients = coefficients.sort_values('coefficient', ascending=False)\nprint(coefficients)\n```\n"
    },
    "6": {
        "step": 6,
        "objective": "6. Logistic regression (using binary income classification):\n   - Convert income into a binary classification variable (e.g., 0 for '<=50K', 1 for '>50K').\n   - Fit a logistic regression model on the dataset with the binary income variable as the target variable and all other variables as predictors.\n   - Evaluate the model performance (e.g., accuracy, precision, recall, F1 score) and analyze the coefficients of each predictor variable. This will help identify the most significant predictors affecting income inequality.",
        "code_block": "```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Separate the predictor variables (X) from the target variable (y)\nX = df.drop('income', axis=1)\ny = df['income']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit a logistic regression model on the dataset\nlogistic_model = LogisticRegression(max_iter=1000)\nlogistic_model.fit(X_train, y_train)\n\n# Predict income values for the test set\ny_pred = logistic_model.predict(X_test)\n\n# Evaluate the model performance\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Analyze the coefficients of each predictor variable\ncoefficients = pd.DataFrame({'feature': X.columns, 'coefficient': logistic_model.coef_[0]})\ncoefficients = coefficients.sort_values('coefficient', ascending=False)\nprint(coefficients)\n```\n"
    },
    "7": {
        "step": 7,
        "objective": "7. Chi-square Test of Independence:\n   - Hypothesis: There is a relationship between income and categorical variables like workclass, education, marital.status, occupation, relationship, race, sex, and native.country.\n   - Create a contingency table for each categorical variable with income levels.\n   - Perform a Chi-square test of independence to assess the relationship between income and each categorical variable. This will help in identifying if there is a significant relationship between income levels and the different categories within each categorical variable.",
        "code_block": "```python\n# Convert the binary income column back to the original categories in the original dataframe\noriginal_df['income'] = original_df['income'].apply(lambda x: '<=50K' if x == 0 else '>50K')\n\n# Perform the Chi-square test of independence for each categorical variable\nchi_square_results = {}\nfor var in categorical_vars:\n    contingency_table = pd.crosstab(original_df[var], original_df['income'])\n    chi_square_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n    chi_square_results[var] = (chi_square_stat, p_value)\n\n# Display the Chi-square test results\nfor var, result in chi_square_results.items():\n    print(f\"{var}: Chi-square Stat:{result[0]:.2f}, P-value:{result[1]:.2e}\")\n```\n"
    }
}